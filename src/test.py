"""
ç›´æ¥ä» test_models ç›®å½•ä¸‹åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œæµ‹è¯•çš„ä»£ç æ–‡ä»¶

æ³¨æ„å‚æ•°ä¸­çš„æ–‡ä»¶åç§°ä¸º test_models ç›®å½•ä¸‹çš„åç§°ï¼Œä½¿ç”¨å‰è¯·æ³¨æ„æ£€æŸ¥è¯¥ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨å¯¹åº”çš„æ–‡ä»¶

æµ‹è¯•é›†æˆæ¨¡å‹æ—¶ï¼ŒæŒ‰å®éªŒä¸­æµ‹è¯•å‡ºçš„æµ‹è¯•å‡†ç¡®æœ€é«˜çš„æ¨¡å‹ç»„åˆè¿›è¡Œæµ‹è¯•ï¼Œä¸æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ç»„åˆ

æ³¨æ„ï¼šè¾“å…¥çš„æ¨¡å‹æ¶æ„å¿…é¡»ä¸æ¨¡å‹æ–‡ä»¶ç›¸åŒ¹é…ï¼å¦åˆ™ç¨‹åºåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¼šæŠ¥é”™ï¼

ä½¿ç”¨ç¤ºä¾‹:
    # å•æ¨¡å‹æµ‹è¯•
    python test.py --model resnet50 --file resnet50.pth

    # é›†æˆæ¨¡å‹æµ‹è¯•
    python test.py --ensemble   # --ensemble å‚æ•°ä¼šè¦†ç›–å…¶ä»–å‚æ•°
"""

import torch
import argparse
import os
import warnings
import numpy as np

warnings.filterwarnings('ignore')

from config import Config
from data_loader import DataManager
from models import get_model
from trainer import Trainer
from utils import evaluate_model, plot_confusion_matrix, plot_class_performance, print_model_info

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # å•æ¨¡å‹æµ‹è¯•
    python test.py --model resnet50 --file resnet50.pth
    # é›†æˆæ¨¡å‹æµ‹è¯•
    python test.py --ensemble
"""
    )

    # æ¨¡å‹æ¶æ„
    parser.add_argument('--model', type=str, default='resnet50',
                        choices=['resnet18', 'resnet34', 'resnet50', 'resnet101',
                               'efficientnet_b0', 'efficientnet_b1', 'custom_cnn',
                               'efficientnet_b2', 'inception_v3', 'vgg11', 'vgg13', 'vgg16'],
                        help='é€‰æ‹©æ¨¡å‹æ¶æ„')
    # æ¨¡å‹æ–‡ä»¶åç§°
    parser.add_argument('--file', type=str, default='resnet50.pth',
                        help='test_models ç›®å½•ä¸‹çš„æ¨¡å‹æ–‡ä»¶åç§°')
    # é›†æˆæ¨¡å‹æµ‹è¯•
    parser.add_argument('--ensemble', action='store_true',
                        help='æ˜¯å¦è¯„ä¼°é›†æˆæ¨¡å‹ï¼Œå¯ç”¨è¯¥å‚æ•°åä¼šè¦†ç›–å…¶ä»–å‚æ•°')
    
    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å²©çŸ³å›¾åƒåˆ†ç±»ç³»ç»Ÿ v2.0")
    print("=" * 70)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    model_type = args.model
    file = args.file
    ensemble = args.ensemble

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"\nâš™ï¸  é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹: {model_type}")
    print(f"   æ¨¡å‹æ–‡ä»¶åç§°ï¼š{file}")

    # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
    if not os.path.exists(os.path.join('test_models', file)):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿è¾“å…¥äº†æ­£ç¡®çš„åç§°")
        return None
    
    # è®¾å¤‡ä¿¡æ¯
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    try:
        # åŠ è½½æ•°æ®
        print("\nğŸ“Š åŠ è½½æ•°æ®...")
        dataManager = DataManager()
        train_loader, valid_loader, test_loader, class_names = dataManager.get_data_loaders()

        print(f"   æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset):,}")
        print(f"   ç±»åˆ«æ•°é‡: {len(class_names)}")

        if ensemble:
            models = {}
            for i, model_type in enumerate(Config.ENSEMBLE_MODELS):
                # åˆ›å»ºæ¨¡å‹
                model = get_model(
                    model_type=model_type,
                    num_classes=len(class_names),
                    pretrained=Config.PRETRAINED,
                    dropout_rate=Config.DROPOUT_RATE
                )

                filename = model_type + '.pth'
                filepath = os.path.join('test_models', filename)
                if os.path.exists(filepath):
                    checkpoint = torch.load(filepath, map_location=device, weights_only=False)
                    model.load_state_dict(checkpoint['model_state_dict'])
                    print(f'æ¨¡å‹å·²ä» {filepath} åŠ è½½')
                else:
                    print(f'æ¨¡å‹æ–‡ä»¶ {filepath} ä¸å­˜åœ¨')
                    return None

                models[model_type] = model
                
            # é›†æˆé¢„æµ‹
            print(f"\nğŸ¯ é›†æˆé¢„æµ‹...")
            ensemble_predictions = []
            all_targets = []

            # è®¾ç½®æ‰€æœ‰æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            for model in models.values():
                model.eval()
                model.to(device)

            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)

                    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
                    batch_probs = []
                    for model in models.values():
                        output = model(data)
                        probs = torch.softmax(output, dim=1)
                        batch_probs.append(probs.cpu().numpy())

                    # å¹³å‡æ¦‚ç‡
                    avg_probs = np.mean(batch_probs, axis=0)
                    predictions = np.argmax(avg_probs, axis=1)

                    ensemble_predictions.extend(predictions)
                    all_targets.extend(target.cpu().numpy())

            # è®¡ç®—é›†æˆå‡†ç¡®ç‡
            ensemble_acc = sum(p == t for p, t in zip(ensemble_predictions, all_targets)) / len(all_targets)

            # è¯„ä¼°é›†æˆç»“æœ
            print("\nğŸ“ˆ é›†æˆæ¨¡å‹è¯„ä¼°...")
            results = evaluate_model(
                predictions=ensemble_predictions,
                targets=all_targets,
                class_names=class_names,
                save_dir=Config.RESULTS_DIR if Config.SAVE_PLOTS else None
            )
            print(f'æµ‹è¯•å‡†ç¡®ç‡: {ensemble_acc:.4f}')

        else:
            # åˆ›å»ºæ¨¡å‹
            model = get_model(
                model_type=Config.MODEL_TYPE,
                num_classes=len(class_names),
                pretrained=Config.PRETRAINED,
                dropout_rate=Config.DROPOUT_RATE
            )

            filepath = os.path.join('test_models', file)
            if os.path.exists(filepath):
                checkpoint = torch.load(filepath, map_location=device, weights_only=False)
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f'æ¨¡å‹å·²ä» {filepath} åŠ è½½')
            else:
                print(f'æ¨¡å‹æ–‡ä»¶ {filepath} ä¸å­˜åœ¨')
                return None

            # æ‰“å°æ¨¡å‹ä¿¡æ¯
            print_model_info(model)

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=model,
                train_loader=train_loader,
                valid_loader=valid_loader,
                test_loader=test_loader,
                config=Config
            )

            # è¯„ä¼°æ¨¡å‹
            print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
            model.eval()
            test_loss, test_acc, predictions, targets = trainer.test()
            results = evaluate_model(
                predictions=predictions,
                targets=targets,
                class_names=class_names,
                save_dir=Config.RESULTS_DIR if Config.SAVE_PLOTS else None
            )
            print(f'æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}')

        # ç»˜åˆ¶ç»“æœ
        plot_confusion_matrix(
            cm=results['confusion_matrix'],
            class_names=class_names,
            save_path=os.path.join(Config.RESULTS_DIR, 'confusion_matrix.png')
        )

        plot_class_performance(
            precision=results['precision'],
            recall=results['recall'],
            f1=results['f1_score'],
            class_names=class_names,
            save_path=os.path.join(Config.RESULTS_DIR, 'class_performance.png')
        )

    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None
    
if __name__ == '__main__':
    main()